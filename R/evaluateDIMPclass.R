#' @rdname evaluateDIMPclass
#'
#' @title Evaluate DIMPs Classification
#' @description For a given cutpoint (previously estimated with the function
#'     estimateCutPoint), 'evaluateDIMPclass' will return the evaluation of the
#'     classification of DIMPs into two clases: DIMPS from control and DIMPs
#'     from treatment samples.
#' @details  The regulatory methylation signal is also an output from a natural
#'     process that continuously takes place across the ontogenetic development
#'     of the organisms. So, we expect to see methylation signal on natural
#'     ordinary conditions. Here, to distinguish a control methylation signal
#'     from a treatment, three classification models are provided: 1) logistic,
#'     2) Linear Discriminant Analysis (LDA) and 3) Quadratic Discriminant
#'     Analysis (QDA). In particular, four predictor variables can be used:
#'     Hellinger divergence "hdiv", total variation "TV", probability of
#'     potential DIMP "wprob" and DIMP genomic coordinated "pos". Principal
#'     component analysis (PCA) is used to convert a set of observations of
#'     possibly correlated predictor variables into a set of values of linearly
#'     uncorrelated variables (principal components, PCs). The PCs are used as
#'     new, uncorrelated predictor variables for LDA, QDA, and logistic
#'     classifiers.
#'
#'     A classification result with low accuracy and compromising values from
#'     other classification performance indicators (see below) suggest that the
#'     treatment does not induce a significant regulatory signal different
#'     from control.
#'
#' @param LR A list of GRanges objects (LR) including control and treatment
#'     GRanges containing divergence values for each DIMP in the meta-column.
#'     LR is generated by the function 'selectDIMP' Each GRanges object must
#'     correspond to a sample. For example, if  a sample is named 's1', then
#'     this sample can be accessed in the list of GRanges objects as LR$s1.
#' @param control.names Names/IDs of the control samples, which must be include
#'     in thr variable LR.
#' @param treatment.names Names/IDs of the treatment samples, which must be
#'     included in the variable LR.
#' @param column a logical vector for column names for the predictor variables
#'     to be used: Hellinger divergence "hdiv", total variation "TV",
#'     probability of potential DIMP "wprob", and the relative cytosine site
#'     position "pos" in respect to the chromosome where it is located. The
#'     relative position is estimated as (x - x.min)/(x.max - x), where x.min
#'     and x.max are the maximum and minimum for the corresponding chromosome,
#'     repectively. If "wprob = TRUE", then Logarithm base-10 of "wprob" will
#'     be used as predictor in place of "wprob".
#' @param classifier Classification model to use. Option "logistic" applies a
#'     logistic regression model; option "lda" applies a Linear Discriminant
#'     Analysis (LDA); "qda" applies a Quadratic Discriminant Analysis (QDA),
#'     "pca.logistic" applies logistic regression model using the Principal
#'     Component (PCs) estimated with Principal Component Analysis (PCA) as
#'     predictor variables. pca.lda" applies LDA using PCs as predictor
#'     variables, and the option "pca.qda" applies a Quadratic Discriminant
#'     Analysis (QDA) using PCs as predictor variables. 'SVM' applies Support
#'     Vector Machines classifier from R package e1071.
#' @param pval.col Column number for p-value used in the performance
#'     analysis and estimation of the cutpoints. Default: NULL. If NULL it is
#'     assumed that the column is named "wprob".
#' @param n.pc Number of principal components (PCs) to use in the LDA. Only
#'     used if classifier = "pcaLDA". In the current case, the maximun number
#'     of PCs is 4.
#' @param center A logical value indicating whether the variables should be
#'     shifted to be zero centered (same as in 'prcomp' {prcomp}). Only used if
#'     classifier = "pcaLDA".
#' @param scale A logical value indicating whether the variables should be
#'     scaled to have unit variance before the analysis takes place (same as in
#'     'prcomp' {prcomp}). Only used if classifier = "pcaLDA".
#' @param interaction Variable interactions to consider in a logistic
#'     regression model. Any pairwise combination of the variable "hdiv", "TV",
#'     "wprob", and "pos" can be provided. For example: "hdiv:TV", "wprob:pos",
#'     "wprob:TV", etc.
#' @param output Type of output to request: output = c("conf.mat", "mc.val",
#'     "boot.all", "all"). See below.
#' @param prop Proportion to split the dataset used in the logistic regression:
#'     group versus divergence (at DIMPs) into two subsets, training and
#'     testing.
#' @param num.boot Number of bootstrap validations to perform in the evaluation
#'     of the logistic regression: group versus divergence (at DIMPs).
#' @param mc.cores The number of cores to use, i.e. at most how many child
#'     processes will be run simultaneously (see bpapply function from
#'     BiocParallel).
#' @param tasks integer(1). The number of tasks per job. value must be a scalar
#'     integer >= 0L. In this documentation a job is defined as a single call
#'     to a function, such as bplapply, bpmapply etc. A task is the division of
#'     the X argument into chunks. When tasks == 0 (default), X is divided as
#'     evenly as possible over the number of workers (see MulticoreParam from
#'     BiocParallel package).
#' @param cachesize To be use in SVM. Cache memory in MB (default 40).
#' @param tolerance Only used for SVM classifier, tolerance of termination
#'     criterion (default: 0.001)
#' @param svm.kernel The kernel used in training and predicting in SVM
#'     classifier. You might consider changing some of the following
#'     parameters, depending on the kernel type: "linear", "polynomial",
#'     "radial", "sigmoid" (see ?svm).
#' @param seed Random seed used for random number generation.
#' @param verbose if TRUE, prints the function log to stdout
#'
#' @return output = "conf.mat" will perform a logistic regression group versus
#'     divergence (at DIMPs) to evaluate the discrimination between
#'     control-DIMPs and treatment-DIMPs. The evaluation of this classification
#'     is provided through the function 'confusionMatrix' from R package
#'     'caret'. "mc.val" will perform a 'num.boot'-times Monte Carlo
#'     (bootstrap) validation and return a summary. By default function
#'     'confusionMatrix' from R package caret' randomly splits the sample into
#'     two subsets, training and testing, according to the supplied proportion
#'     'prop' (i.e., prop = 0.6). After selecting output = "mc.val", the
#'     function 'confusionMatrix' will be executed 'num.boot'-times, each time
#'     performing a different random split of the sample. "boot.all" same as
#'     "mc.val" plus a matrix with statistcs reported by 'confusionMatrix'.
#'     "all" return a list with all the mentioned outputs.
#'
#' @examples
#' set.seed(123) ## To set a seed for random number generation
#' ## GRanges object of the reference with methylation levels in
#' ## its matacolumn
#' num.points <- 5000
#' Ref <- makeGRangesFromDataFrame(
#'     data.frame(chr = '1',
#'             start = 1:num.points,
#'             end = 1:num.points,
#'             strand = '*',
#'             p1 = rbeta(num.points, shape1 = 1, shape2 = 1.5)),
#'     keep.extra.columns = TRUE)
#'
#' ## List of Granges objects of individuals methylation levels
#' Indiv <- GRangesList(
#'     sample11 = makeGRangesFromDataFrame(
#'         data.frame(chr = '1',
#'             start = 1:num.points,
#'             end = 1:num.points,
#'             strand = '*',
#'             p2 = rbeta(num.points, shape1 = 1.5, shape2 = 2)),
#'          keep.extra.columns = TRUE),
#'     sample12 = makeGRangesFromDataFrame(
#'         data.frame(chr = '1',
#'             start = 1:num.points,
#'             end = 1:num.points,
#'             strand = '*',
#'             p2 = rbeta(num.points, shape1 = 1.6, shape2 = 2)),
#'         keep.extra.columns = TRUE),
#'     sample21 = makeGRangesFromDataFrame(
#'         data.frame(chr = '1',
#'             start = 1:num.points,
#'             end = 1:num.points,
#'             strand = '*',
#'             p2 = rbeta(num.points, shape1 = 40, shape2 = 4)),
#'         keep.extra.columns = TRUE),
#'     sample22 = makeGRangesFromDataFrame(
#'         data.frame(chr = '1',
#'             start = 1:num.points,
#'             end = 1:num.points,
#'             strand = '*',
#'             p2 = rbeta(num.points, shape1 = 41, shape2 = 4)),
#'         keep.extra.columns = TRUE))
#' ## To estimate Hellinger divergence using only the methylation levels.
#' HD <- estimateDivergence(ref = Ref, indiv = Indiv, meth.level = TRUE,
#'                             columns = 1)
#' ## To perform the nonlinear regression analysis
#' nlms <- nonlinearFitDist(HD, column = 4, verbose = FALSE)
#'
#' ## Next, the potential signal can be estimated
#' PS <- getPotentialDIMP(LR = HD, nlms = nlms, div.col = 4, alpha = 0.05)
#'
#' ## The cutpoint estimation used to discriminate the signal from the noise
#' cutpoints <- estimateCutPoint(PS, control.names = c("sample11", "sample12"),
#'                             treatment.names = c("sample21", "sample22"),
#'                             div.col = 4, verbose = TRUE)
#' ## DIMPs are selected using the cupoints
#' DIMPs <- selectDIMP(PS, div.col = 4, cutpoint = min(cutpoints$cutpoint))
#'
#' ## Classification of DIMPs into two clases: DIMPS from control and DIMPs from
#' ## treatment samples and evaluation of the classifier performance (for more
#' ## details see ?evaluateDIMPclass).
#' conf.mat <- evaluateDIMPclass(DIMPs,
#'                             column = c(hdiv = TRUE, TV = TRUE,
#'                             wprob = TRUE, pos = FALSE),
#'                             control.names = c("sample11", "sample12"),
#'                             treatment.names = c("sample21", "sample22"))
#' confusion.matrix <- conf.mat$conf.mat
#' model.fit <- summary(conf.mat$model)
#' @importFrom GenomicRanges GRanges GRangesList
#' @importFrom caret confusionMatrix
#' @importFrom stats binom.test mcnemar.test predict.glm binomial
#' @importFrom BiocParallel MulticoreParam bplapply SnowParam
#' @export
evaluateDIMPclass <- function(LR, control.names, treatment.names,
                               column=c(hdiv=FALSE, TV=FALSE,
                                         wprob=FALSE, pos=FALSE),
                               classifier=c("logistic", "pca.logistic", "lda",
                                             "svm", "qda","pca.lda", "pca.qda"),
                               pval.col=NULL,
                               n.pc=1, center=FALSE, scale=FALSE,
                               interaction=NULL, output="conf.mat",
                               prop=0.6, num.boot=100, mc.cores=1L,
                               tasks=0L, cachesize=250007,
                               tolerance=0.0001,
                               svm.kernel=c("linear", "polynomial",
                                             "radial", "sigmoid"),
                               seed=1234, verbose=TRUE) {

   if (sum(column) == 0)
       stop(paste("*** At least one of columns with the predictor variables",
               " 'hdiv', 'TV', logP, or pos' must be provided"))
   if ((classifier[1] != "logistic" ) && sum(column) < n.pc) {
       stop(paste("* The number of predictor variables must be greater or ",
               "equal to n.pc"))
   }
   set.seed(seed)

   position <- function(gr) {
       chrs <- split(gr, seqnames(gr))
       gr <- lapply(chrs, function(grc) {
               x <- start(grc)
               x.min <- min(x)
               x.max <- max(x)
               delta <-  max(c(x.max - x, 1))
               return((x - x.min) / (delta))})
       return(unlist(gr))}

   DIV <- function(LR) {
       ## This builds data frames from the list or ranges LR
       ## to be used for ROC analysis
       ## LR: list of sample GRanges
       vn <- c("hdiv", "TV", "wprob", "pos")
       if (classifier[1] == "logistic") {
           idx <- unlist(lapply(vn, function(s) sum(grepl(s, interaction)) > 0))
           column[union(vn[column], vn[idx])] <- TRUE
       }

       vn <- vn[column]
       vn <- setdiff(vn, "pos")
       sn <- names(LR)
       dt <- data.frame()
       for (k in 1:length(LR)) {
           dc <- c()
           x <- LR[[k]]
           if (!is.null(pval.col)) x$wprob <- mcols(x[, pval.col])[, 1]
           x <- x[ ,vn]
           if (column["hdiv"]) dc <- cbind(dc, hdiv=x$hdiv)
           if (column["TV"]) dc <- cbind(dc, TV=x$TV)
           if (column["wprob"]) dc <- cbind(dc, logP=log10(x$wprob + 2.2e-308))
           if (column["pos"] || sum(grepl("pos", interaction)) > 0) {
               dc = cbind(dc, pos = position(x))
           }
           dt <- rbind(dt, data.frame(dc, treat=sn[k]))
       }
       return(dt)
   }

   LogistR <- function(dt, formula) {
       ## This function performs a logistic regression
       ## using 'glm'.
       l <- levels(dt$treat)
       dt$treat <- as.character(dt$treat)
       dt$treat[dt$treat == l[1]] <- 0
       dt$treat[dt$treat == l[2]] <- 1
       dt$treat <- as.numeric(dt$treat)
       model <- suppressWarnings(glm(formula, family=binomial(link='logit'),
                               data=dt))
       return(model)
   }

   ## ======= To build the regression formula ======== ##
   vn <- c("hdiv", "TV", "logP", "pos")
   cn <- column; names(cn) <- c("hdiv", "TV", "logP", "pos")
   form <- as.character(outer(vn, vn, FUN=paste, sep = ":"))
   form <- form[c(2:5, 7:10, 12:15)]
   inter <- c("hdiv:TV" = FALSE, "hdiv:logP" = FALSE, "hdiv:pos" = FALSE,
               "TV:hdiv" = FALSE, "TV:logP" = FALSE, "TV:pos" = FALSE,
               "logP:hdiv" = FALSE, "logP:TV" = FALSE, "logP:pos" = FALSE,
               "pos:hdiv" = FALSE, "pos:TV" = FALSE, "pos:logP" = FALSE)
   if (!is.null(interaction) && classifier[1] == "logistic") {
       if (sum(grepl("wprob", interaction)) > 0) {
           idx <- sub("wprob", "logP", interaction)
           inter[idx] <- TRUE
       } else inter[interaction] <- TRUE
   }

   if (sum(inter) > 0 && classifier[1] == "logistic") {
       predictors <- paste(paste0(vn=vn[cn], collapse=" + "),
                       paste0(form[inter], collapse=" + "), sep =" + ")
       form <- paste0("treat ~ ", predictors)
       formula <- as.formula(form)
   } else {
       form <- paste0("treat ~ ", paste(vn[cn], collapse=" + "))
       formula <- as.formula(form)}
   if (verbose) message("Model: ", form)
   ## ======== data preprocessing ======== ##
   sn <- names(LR)
   idx.ct <- match(control.names, sn)
   idx.tt <- match(treatment.names, sn)
   CT <- GRangesList(LR[idx.ct])
   CT <- unlist(CT)
   TT <- GRangesList(LR[idx.tt])
   TT <- unlist(TT)

  ## ======== computation =========== ##
   conf.mat <- function(k) {
       lct <- length(CT)
       ltt <- length(TT)
       lct <- sample.int(lct, size=(lct * prop))
       ltt <- sample.int(ltt, size=(ltt * prop))

       trainingSet <- list()
       testSet <- list()
       trainingSet$CT <- CT[lct]
       trainingSet$TT <- TT[ltt]
       testSet$CT <- CT[-lct]
       testSet$TT <- TT[-ltt]

       trainingSet <- DIV(trainingSet)
       testSet <- DIV(testSet)
       model <- switch(classifier[1],
                       logistic=LogistR(trainingSet, formula=formula),
                       pca.logistic=pcaLogisticR(formula=formula,
                                               data=trainingSet, n.pc=n.pc,
                                               scale=scale, center=center,
                                               max.pc=4),
                       lda=lda(formula=formula, data=trainingSet,
                               tol=1.0e-4),
                       qda=qda(formula=formula, data=trainingSet,
                               tol=1.0e-4),
                       svm=svm(formula=formula, data=trainingSet,
                               kernel=svm.kernel[1], cachesize=cachesize,
                               scale=scale, center = center, probability=TRUE,
                               type="C-classification", tolerance=tolerance),
                       pca.lda=pcaLDA(formula=formula,
                               data=trainingSet, n.pc=n.pc,
                               max.pc=4, scale=scale,
                               center=center),
                       pca.qda=pcaQDA(formula=formula,
                               data=trainingSet, n.pc=n.pc,
                               max.pc=4, scale=scale,
                               center=center))
       PredTestSet <- switch(classifier[1],
                       logistic=predict.glm(model, newdata=testSet,
                                                type="response"),
                       lda=predict(model,
                               newdata=testSet)$posterior[,"TT"],
                       qda=predict(model,
                               newdata=testSet)$posterior[,"TT"],
                       svm=attributes(predict(model, newdata=testSet,
                               probability = TRUE))$prob[,"TT"],
                       pca.logistic=predict(model, newdata=testSet,
                               type="response"),
                       pca.lda=predict(model, newdata=testSet[ ,vn[cn]],
                               type="posterior")[,"TT"],
                       pca.qda=predict(model, newdata=testSet[ ,vn[cn]],
                               type="posterior")[ ,"TT"])
       PredTestClass <- rep( "CT", length(PredTestSet))
       PredTestClass[PredTestSet > 0.5] <- "TT"
       PredTestClass <- factor(PredTestClass)
       if (length(levels(PredTestClass)) < 2 ) {
           warning(paste0("Predictions only have one classification level, ",
                   levels(PredTestClass),
                   ", consider increasing the proportion to split the ",
                   "dataset or to vary the number of predictor variables \n"))
       }
       conf.mat=confusionMatrix(data=PredTestClass, reference=testSet$treat,
                                positive="TT")
       m=conf.mat$table
       FDR=m[2,1]/sum(m[2,])
       return(list(Performance=conf.mat, FDR=FDR, model=model))
   }

   if (output != "conf.mat") {
       if (.Platform$OS.type == "unix") {
           bpparam <- MulticoreParam(workers=mc.cores, tasks=tasks)
       } else bpparam <- SnowParam(workers=mc.cores)
       boots <- bplapply(1:num.boot, function(k){
               x <- conf.mat(k)$Performance
               return(c(x$overall, x$byClass))}, BPPARAM=bpparam)

       boots <- do.call(rbind,boots)
   }
   res <- switch(output, conf.mat=conf.mat(1), mc.val=summary(boots),
               boot.all=list(stats=summary(boots), boots=boots),
               all=list(con.mat=conf.mat(1), mc.val=summary(boots),
                       boots=boots))
   return(res)
}
